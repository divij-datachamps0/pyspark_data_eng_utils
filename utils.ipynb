{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "867e77c5-3d9e-4e2a-9616-f8a126c04619",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "README"
    }
   },
   "outputs": [],
   "source": [
    "### Common Utilities Notebook\n",
    "\n",
    "# Author: Divij Kulshrestha\n",
    "# Last updated: 2026-02-05\n",
    "# Version: 1.1\n",
    "\n",
    "\n",
    "### README\n",
    "#### This notebook contains utility functions for data engineering workflows.\n",
    "#### This notebook can be called by any other notebooks in the same Workspace\n",
    "#### Use the %run magic command to call /Shared/utils at the start of your notebook to inherit these UDFs\n",
    "#### You do not need to import the libraries that have already been called in this notebook\n",
    "#### Certain parameters like file paths and secret keys are Workspace-specific. Please check before using in another Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7a7d0b-d001-4963-b147-3d37254e7976",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Libraries"
    }
   },
   "outputs": [],
   "source": [
    "#Pyspark Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, TimestampType, DoubleType, StructType, StructField\n",
    ")\n",
    "\n",
    "#Python Libraries\n",
    "import os\n",
    "import uuid, time, traceback\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# For IST Timezone\n",
    "try:\n",
    "    from zoneinfo import ZoneInfo\n",
    "    _IST = ZoneInfo(\"Asia/Kolkata\")\n",
    "except Exception:\n",
    "    try:\n",
    "        import pytz\n",
    "        _IST = pytz.timezone(\"Asia/Kolkata\")\n",
    "    except Exception:\n",
    "        _IST = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb3e839-722c-4c64-bee3-c55f8d23eed2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ADLS OAuth Connector"
    }
   },
   "outputs": [],
   "source": [
    "# Function to connect to ADLS\n",
    "# \"akp_connector\" in Gera projects\n",
    "\n",
    "def adls_connector(accountName, fileSystemName, idKey, secretKey, tenant_id):\n",
    "    \"\"\"\n",
    "    Sets Spark Hadoop configuration entries to use OAuth (Client Credentials) for Azure ADLS Gen2.\n",
    "    Builds and returns an ABFSS URI string\n",
    "    \n",
    "    Args:\n",
    "        accountName: ADLS storage account name\n",
    "        fileSystemName: Typically ADLS Gen2 file system name\n",
    "        idKey: OAuth client ID\n",
    "        secretKey: OAuth client secret\n",
    "        tenant_id: OAuth tenant ID\n",
    "    \n",
    "    Returns:\n",
    "        string: ABFSS URI string pointing to root in ADLS\n",
    "    \"\"\"\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{accountName}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{accountName}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{accountName}.dfs.core.windows.net\", idKey)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{accountName}.dfs.core.windows.net\", secretKey)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{accountName}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "    spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n",
    "    return f\"abfss://{fileSystemName}@{accountName}.dfs.core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4160f5b-081a-4894-8029-6ca2c0ab5c19",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logging to Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, Dict, List\n",
    "import pandas as pd\n",
    "import builtins  # Import builtins to access original round function\n",
    "\n",
    "class NotebookTimer:\n",
    "    \"\"\"Track execution time for notebook code blocks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.blocks: Dict[str, Dict] = {}\n",
    "        self.current_block: Optional[str] = None\n",
    "        self.start_time: Optional[float] = None\n",
    "        self.notebook_start: Optional[float] = None\n",
    "        \n",
    "    def start_notebook(self):\n",
    "        \"\"\"Start tracking the overall notebook execution.\"\"\"\n",
    "        self.notebook_start = time.time()\n",
    "        print(f\"ðŸ“Š Notebook execution started at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "    def start_block(self, block_name: str):\n",
    "        \"\"\"Start timing a new code block.\"\"\"\n",
    "        # End previous block if one is running\n",
    "        if self.current_block:\n",
    "            self.end_block()\n",
    "            \n",
    "        self.current_block = block_name\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        if block_name not in self.blocks:\n",
    "            self.blocks[block_name] = {\n",
    "                'executions': [],\n",
    "                'total_time': 0\n",
    "            }\n",
    "        \n",
    "        print(f\"\\nâ±ï¸  Starting: {block_name}\")\n",
    "        \n",
    "    def end_block(self):\n",
    "        \"\"\"End timing the current code block.\"\"\"\n",
    "        if not self.current_block or not self.start_time:\n",
    "            print(\"âš ï¸  No active block to end\")\n",
    "            return\n",
    "            \n",
    "        duration = time.time() - self.start_time\n",
    "        self.blocks[self.current_block]['executions'].append(duration)\n",
    "        self.blocks[self.current_block]['total_time'] += duration\n",
    "        \n",
    "        print(f\"âœ… Completed: {self.current_block} ({self._format_duration(duration)})\")\n",
    "        \n",
    "        self.current_block = None\n",
    "        self.start_time = None\n",
    "        \n",
    "    def end_notebook(self, output: str = \"print\"):\n",
    "        \"\"\"End notebook tracking and display summary.\n",
    "        \n",
    "        Args:\n",
    "            output: \"print\" for console summary, \"table\" or \"df\" for DataFrame output,\n",
    "                   \"both\" for both outputs\n",
    "        \"\"\"\n",
    "        # End any active block\n",
    "        if self.current_block:\n",
    "            self.end_block()\n",
    "            \n",
    "        if not self.notebook_start:\n",
    "            print(\"âš ï¸  Notebook timer was not started\")\n",
    "            return None\n",
    "            \n",
    "        total_time = time.time() - self.notebook_start\n",
    "        \n",
    "        # Print summary\n",
    "        if output in [\"print\", \"both\"]:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ðŸ“ˆ EXECUTION SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Display individual blocks\n",
    "            for block_name, data in self.blocks.items():\n",
    "                avg_time = data['total_time'] / len(data['executions'])\n",
    "                pct = (data['total_time'] / total_time) * 100\n",
    "                \n",
    "                print(f\"\\n{block_name}:\")\n",
    "                print(f\"  Duration: {self._format_duration(data['total_time'])}\")\n",
    "                print(f\"  Percentage: {pct:.1f}%\")\n",
    "                if len(data['executions']) > 1:\n",
    "                    print(f\"  Executions: {len(data['executions'])} (avg: {self._format_duration(avg_time)})\")\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(f\"ðŸ• Total Notebook Time: {self._format_duration(total_time)}\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        # Return DataFrame\n",
    "        if output in [\"table\", \"df\", \"both\"]:\n",
    "            return self.get_summary_df()\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def get_summary_df(self) -> pd.DataFrame:\n",
    "        \"\"\"Return execution summary as a DataFrame.\"\"\"\n",
    "        if not self.notebook_start:\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        total_time = time.time() - self.notebook_start\n",
    "        \n",
    "        data = []\n",
    "        for block_name, block_data in self.blocks.items():\n",
    "            data.append({\n",
    "                'Section': block_name,\n",
    "                'Time Taken': self._format_duration(block_data['total_time']),\n",
    "                'Time (seconds)': builtins.round(block_data['total_time'], 2),\n",
    "                'Percentage': f\"{(block_data['total_time'] / total_time) * 100:.1f}%\"\n",
    "            })\n",
    "        \n",
    "        # Add total row\n",
    "        data.append({\n",
    "            'Section': 'TOTAL',\n",
    "            'Time Taken': self._format_duration(total_time),\n",
    "            'Time (seconds)': builtins.round(total_time, 2),\n",
    "            'Percentage': '100.0%'\n",
    "        })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    \n",
    "    def _format_duration(self, seconds: float) -> str:\n",
    "        \"\"\"Format duration in human-readable format.\"\"\"\n",
    "        if seconds < 1:\n",
    "            return f\"{seconds*1000:.0f}ms\"\n",
    "        elif seconds < 60:\n",
    "            return f\"{seconds:.2f}s\"\n",
    "        else:\n",
    "            minutes = int(seconds // 60)\n",
    "            secs = seconds % 60\n",
    "            return f\"{minutes}m {secs:.1f}s\"\n",
    "\n",
    "\n",
    "# Create global timer instance\n",
    "timer = NotebookTimer()\n",
    "\n",
    "# Convenience functions\n",
    "def start_notebook():\n",
    "    \"\"\"Start tracking notebook execution.\"\"\"\n",
    "    timer.start_notebook()\n",
    "\n",
    "def start_block(name: str):\n",
    "    \"\"\"Start timing a code block.\"\"\"\n",
    "    timer.start_block(name)\n",
    "\n",
    "def end_block():\n",
    "    \"\"\"End timing current code block.\"\"\"\n",
    "    timer.end_block()\n",
    "\n",
    "def end_notebook(output: str = \"print\"):\n",
    "    \"\"\"End notebook and show summary.\n",
    "    \n",
    "    Args:\n",
    "        output: \"print\" for console summary (default)\n",
    "                \"table\" or \"df\" for DataFrame output\n",
    "                \"both\" for both outputs\n",
    "    \"\"\"\n",
    "    return timer.end_notebook(output)\n",
    "\n",
    "def summary():\n",
    "    \"\"\"Display summary as DataFrame.\"\"\"\n",
    "    return timer.get_summary_df()\n",
    "\n",
    "\n",
    "# USAGE EXAMPLE:\n",
    "# ================\n",
    "\n",
    "# At the start of your notebook:\n",
    "# start_notebook()\n",
    "\n",
    "# Before each code block:\n",
    "# start_block(\"Setup\")\n",
    "# ... your setup code ...\n",
    "\n",
    "# start_block(\"Load Data\")\n",
    "# ... your data loading code ...\n",
    "\n",
    "# start_block(\"ETL Transforms - A\")\n",
    "# ... your ETL code ...\n",
    "\n",
    "# start_block(\"ETL Transforms - B\")\n",
    "# ... more ETL code ...\n",
    "\n",
    "# start_block(\"Write Data\")\n",
    "# ... your write code ...\n",
    "\n",
    "# At the end of your notebook:\n",
    "\n",
    "# Option 1: Print summary to console (default)\n",
    "# end_notebook()\n",
    "\n",
    "# Option 2: Get DataFrame for CSV export\n",
    "# df = end_notebook(\"dataframe\")\n",
    "# df.to_csv(\"execution_report.csv\", index=False)\n",
    "\n",
    "# Option 3: Get both print and DataFrame\n",
    "# df = end_notebook(\"both\")\n",
    "# df.to_csv(\"execution_report.csv\", index=False)\n",
    "\n",
    "# Option 4: Get DataFrame anytime\n",
    "# df = summary()\n",
    "# df.to_csv(\"execution_report.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def time_block(label: str):\n",
    "    \"\"\"\n",
    "    Context manager to measure and log the elapsed execution time of a code block.\n",
    "\n",
    "    This utility is intended for lightweight performance diagnostics and can be\n",
    "    used in notebooks, batch jobs, Azure Functions, or any standard Python runtime.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    label : str\n",
    "        A descriptive name for the code block being measured. This label will\n",
    "        be included in the log output to help identify the timed section.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    None\n",
    "        Control is yielded to the wrapped code block. Upon exit, the elapsed\n",
    "        time is calculated and printed to standard output.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> with time_block(\"load_exchange_rates\"):\n",
    "    ...     df = spark.read.table(\"exchange_rates\")\n",
    "    ...     df.count()\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    [load_exchange_rates] Elapsed: 2.41s\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        elapsed = round(time.time() - start, 2)\n",
    "        print(f\"[{label}] Elapsed: {elapsed}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9114d5f2-1412-419f-b01d-b97a6888f2cb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Logging to CSV"
    }
   },
   "outputs": [],
   "source": [
    "class NotebookLogger:\n",
    "    def __init__(self, log_filename=\"default_log.csv\", path=\"/mnt/projects/logs/\"):\n",
    "        \"\"\"\n",
    "        Initialize a lightweight CSV logger for Databricks notebooks.\n",
    "        \n",
    "        Args:\n",
    "            log_filename (str): Log file name (default: 'default_log.csv')\n",
    "            path (str): ADLS mount path (default: '/mnt/projects/logs/')\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.log_filename = log_filename\n",
    "        self.full_path = os.path.join(path, log_filename)\n",
    "        \n",
    "        # Ensure the log file exists with header\n",
    "        try:\n",
    "            dbutils.fs.head(self.full_path)\n",
    "        except:\n",
    "            header = \"timestamp,level,message\\n\"\n",
    "            dbutils.fs.put(self.full_path, header, overwrite=True)\n",
    "    \n",
    "    def log(self, message, level=\"INFO\"):\n",
    "        \"\"\"Append a timestamped log entry to the CSV file.\"\"\"\n",
    "       # Produce timestamp in IST (Asia/Kolkata). Fallback to local time if tz libs unavailable.\n",
    "        if _IST is not None:\n",
    "            timestamp = datetime.now(_IST).strftime(\"%Y-%m-%d %H:%M:%S %z\")\n",
    "        else:\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        line = f\"{timestamp},{level},{message}\\n\"\n",
    "        #dbutils.fs.put(self.full_path, line, overwrite=False)\n",
    "        try:\n",
    "            existing = dbutils.fs.head(self.full_path)\n",
    "        except:\n",
    "            existing = \"\"\n",
    "        dbutils.fs.put(self.full_path, existing + line, overwrite=True)\n",
    "    \n",
    "    def info(self, message):\n",
    "        self.log(message, level=\"INFO\")\n",
    "    \n",
    "    def warn(self, message):\n",
    "        self.log(message, level=\"WARN\")\n",
    "    \n",
    "    def error(self, message):\n",
    "        self.log(message, level=\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f764f88c-5942-4b1b-9f98-a3e3e53b1c3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write Single CSV to ADLS"
    }
   },
   "outputs": [],
   "source": [
    "# Function to coalesce and write a single CSV file to ADLS (not a folder)\n",
    "def write_single_csv(df, base_path, base_filename, add_timestamp=True, container=None, storage_account=None, sep=\",\", header=True, overwrite=True):\n",
    "    \"\"\"\n",
    "    Writes a Spark DataFrame as a single CSV file with optional timestamped name in ADLS (via abfss:// path).\n",
    "    Will overwrite same file name by default.\n",
    "\n",
    "    Example Usage: write_single_csv(df, ZONE2, \"filename\")\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Spark DataFrame\n",
    "    - base_path: Path prefix (e.g., 'abfss://container@account.dfs.core.windows.net/intermediate')\n",
    "    - base_filename: Base name for output file (e.g., 'buildingwise_combined')\n",
    "    - add_timestamp: Whether to add timestamp to filename (default: True)\n",
    "    - container, storage_account: (Optional) only needed if base_path is a relative ZONE path\n",
    "    - sep: Field separator (default: ',')\n",
    "    - header: Whether to include header (default: True)\n",
    "    - overwrite: Overwrite mode (default: True)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate filename with optional timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_filename = f\"{base_filename}_{timestamp}.csv\" if add_timestamp else f\"{base_filename}.csv\"\n",
    "    temp_path = f\"{base_path}/temp_{timestamp}\"\n",
    "    final_path = f\"{base_path}/{final_filename}\"\n",
    "\n",
    "    # Write DataFrame as single-part CSV to temp folder\n",
    "    writer = df.coalesce(1).write.option(\"sep\", sep).option(\"header\", str(header).lower())\n",
    "    if overwrite:\n",
    "        writer = writer.mode(\"overwrite\")\n",
    "    writer.csv(temp_path)\n",
    "\n",
    "    # Find the single part file\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    part_file = [f.path for f in files if f.name.startswith(\"part-\")][0]\n",
    "\n",
    "    # Move (rename) to final destination\n",
    "    dbutils.fs.mv(part_file, final_path)\n",
    "\n",
    "    # Clean up the temporary folder\n",
    "    dbutils.fs.rm(temp_path, True)\n",
    "\n",
    "    #return final_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "07a03e9e-09d7-42e2-b580-26b4f179263c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Safe Read CSV"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import traceback\n",
    "\n",
    "def safe_read_csv(path: str, \n",
    "                  header=True, \n",
    "                  inferSchema=True, \n",
    "                  expected_cols: list[str] = None,  \n",
    "                  mandatory=True) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Safely reads a CSV file with error handling and schema validation.\n",
    "    Logs errors for empty files, schema mismatches, and read failures.\n",
    "    Requires NotebookLogger implementation.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Full path of the CSV file.\n",
    "        header (bool): Whether the CSV has headers. Default = True.\n",
    "        inferSchema (bool): Whether to infer column types. Default = True.\n",
    "        expected_cols (list): List of expected column names (optional).\n",
    "        mandatory (bool): If True, raises an exception on failure instead of continuing.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Spark DataFrame (empty if read fails).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(path, header=header, inferSchema=inferSchema)\n",
    "        if df.rdd.isEmpty():\n",
    "            logger.warn(f\"File at {path} is empty.\")\n",
    "            if mandatory:\n",
    "                raise ValueError(f\"Empty file at {path} (mandatory file).\")\n",
    "        \n",
    "        if not all(col in df.columns for col in expected_cols):\n",
    "            logger.warn(f\"Unexpected schema in {path}: {df.columns}\")\n",
    "            if mandatory:\n",
    "                raise ValueError(f\"Unexpected schema in {path}: {df.columns}\")  \n",
    "\n",
    "        logger.info(f\"Successfully read file: {path} with {df.count()} rows.\")\n",
    "        return df\n",
    "    except AnalysisException as e:\n",
    "        logger.warn(f\"File not found or access issue at {path}: {str(e)}\")\n",
    "        if mandatory:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {path}: {traceback.format_exc()}\")\n",
    "        if mandatory:\n",
    "            raise e\n",
    "\n",
    "    # Return an empty DataFrame with no schema to allow continuation\n",
    "    return spark.createDataFrame([], schema=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ef47ab47-1784-4f50-b41d-1ba673434bf7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformations"
    }
   },
   "outputs": [],
   "source": [
    "# Function to average non-zero rows in a column\n",
    "def avg_nonzero(col_name, alias=None):\n",
    "    \"\"\"\n",
    "    Return a Column that computes the average of non-zero values in a column.\n",
    "\n",
    "    Zeros are treated as missing (excluded) by wrapping the column in a when(...).\n",
    "    Existing nulls are ignored by pyspark.functions.avg. The resulting Column is\n",
    "    aliased to `alias` (defaults to the original column name).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_name : str\n",
    "        Name of the column to average.\n",
    "    alias : str, optional\n",
    "        Alias for the resulting Column. If None, `col_name` is used.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pyspark.sql.Column\n",
    "        A Column expression computing the average of non-zero values (null if no\n",
    "        non-zero values are present).\n",
    "    \"\"\"    \n",
    "    alias = alias or col_name\n",
    "    return F.avg(F.when(F.col(col_name) != 0, F.col(col_name))).alias(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eab93d9-7845-439b-9656-aeb059b37b83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "General"
    }
   },
   "outputs": [],
   "source": [
    "# Spark - DataFrame Utilities\n",
    "\n",
    "# Function to get dataframe details (rows, cols)\n",
    "def get_shape(df) -> str:\n",
    "    return f\"rows: {df.count()} | cols: {len(df.columns)}\"\n",
    "\n",
    "\n",
    "# Return Dataframe Stats\n",
    "def df_stats(df):\n",
    "    return {\n",
    "        \"rows\": df.count(),\n",
    "        \"partitions\": df.rdd.getNumPartitions(),\n",
    "        \"schema\": df.schema.simpleString()\n",
    "    }\n",
    "\n",
    "def check_column_exists(df, column_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a column exists in the given DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame to check\n",
    "        column_name: Name of the column to look for (case-sensitive)\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if column exists, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return column_name in df.columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking column {column_name}: {e}\")\n",
    "        return False\n",
    "    \n",
    "def compare_distinct_cardinality(df1, df2, col_name, col_name2=None):\n",
    "    \"\"\"\n",
    "    Compare the distinct-value cardinality of a column between two DataFrames.\n",
    "\n",
    "    Parameters\n",
    "    - df1 (pyspark.sql.DataFrame): first DataFrame.\n",
    "    - df2 (pyspark.sql.DataFrame): second DataFrame.\n",
    "    - col_name (str): column name in df1 to compare.\n",
    "    - col_name2 (str, optional): column name in df2 to compare. If None, col_name is used.\n",
    "\n",
    "    Returns\n",
    "    - bool: True if the number of distinct values in df1[col_name] equals the number in df2[col_name2], else False.\n",
    "\n",
    "    Notes\n",
    "    - Uses distinct().count(), which triggers Spark jobs and can be expensive on large datasets.\n",
    "    - Column names are used as provided; pass col_name2 when the column name differs in df2 (e.g., \"Year\" vs \"YEAR\").\n",
    "    \"\"\"\n",
    "    col2 = col_name if col_name2 is None else col_name2\n",
    "    return df1.select(F.col(col_name)).distinct().count() == df2.select(F.col(col2)).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d6d52d5-d541-4511-9c78-5f72f70fc3df",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "standardize_columns"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def underscore(word):\n",
    "    \"\"\"\n",
    "    Make an underscored, lowercase form from the expression in the string.\n",
    "    CamelCase words get undone into the lower_snake_case (standard for DataFrame columns in PySpark).\n",
    "\n",
    "    Example::\n",
    "\n",
    "        >>> underscore(\"DeviceType\")\n",
    "        'device_type'\n",
    "\n",
    "    Inspired by PyPI Project Inflection maintained by jpvanhal.\n",
    "    \"\"\"\n",
    "    word = re.sub(r\"([A-Z]+)([A-Z][a-z])\", r'\\1_\\2', word)\n",
    "    word = re.sub(r\"([a-z\\d])([A-Z])\", r'\\1_\\2', word)\n",
    "    word = word.replace(\"-\", \"_\")\n",
    "    word = word.replace(\" \", \"_\")\n",
    "    return word.lower()\n",
    "\n",
    "\n",
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Rename columns from CamelCase or Mixed Case into a standardized lower_snake_case.\n",
    "    Column Names like \"AvgHeight\" or \"Avg Height\" will become \"avg_height\".\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        df = df.withColumnRenamed(c, inflection.underscore(c).lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_event(run_id, pipeline_name, notebook_name, status, start_time=None, end_time=None, message=None):\n",
    "    \"\"\"\n",
    "    Logs an event to a central admin/etl job run table.\n",
    "\n",
    "    Requires a central Delta Table already created.\n",
    "    \n",
    "    Args:\n",
    "        run_id (str): Can be parametrized to take from Azure Data Factory\n",
    "        pipeline_name (str): Name of the pipeline (from Azure Data Factory)\n",
    "        notebook_name (str): Name of the notebook (hard code in your notebook)\n",
    "        status (str): Status of the event (STARTED/SUCCESS/FAILURE)\n",
    "        start_time (datetime): Start time of the event (default: None)\n",
    "        end_time (datetime): End time of the event (default: None)\n",
    "        message (str): Message associated with the event (default: None)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    \n",
    "    duration = None\n",
    "    if start_time and end_time:\n",
    "        duration = int(end_time.timestamp() - start_time.timestamp())\n",
    "\n",
    "    #data = [(str(uuid.uuid4()), run_id, pipeline_name, notebook_name, status,\n",
    "             #start_time, end_time, duration, message, None)]\n",
    "    \n",
    "    data = [(\n",
    "        str(uuid.uuid4()),\n",
    "        str(run_id),\n",
    "        str(pipeline_name),\n",
    "        str(notebook_name),\n",
    "        str(status),\n",
    "        start_time,   \n",
    "        end_time,\n",
    "        float(duration) if duration is not None else None,\n",
    "        str(message) if message is not None else None,\n",
    "        None\n",
    "    )]\n",
    "    \n",
    "    columns = [\"log_id\", \"run_id\", \"pipeline_name\", \"notebook_name\", \"status\",\n",
    "               \"start_time\", \"end_time\", \"duration_sec\", \"message\", \"created_at\"]\n",
    "    \n",
    "    schema = StructType([\n",
    "            StructField(\"log_id\", StringType(), False),\n",
    "            StructField(\"run_id\", StringType(), True),\n",
    "            StructField(\"pipeline_name\", StringType(), True),\n",
    "            StructField(\"notebook_name\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"start_time\", TimestampType(), True),\n",
    "            StructField(\"end_time\", TimestampType(), True),\n",
    "            StructField(\"duration_sec\", DoubleType(), True),\n",
    "            StructField(\"message\", StringType(), True),\n",
    "            StructField(\"created_at\", TimestampType(), True)\n",
    "        ])\n",
    "        \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    (df.withColumn(\"created_at\", F.from_utc_timestamp(F.current_timestamp(), \"Asia/Kolkata\"))\n",
    "       .withColumn(\"start_time\", F.from_utc_timestamp(F.col(\"start_time\"), \"Asia/Kolkata\"))\n",
    "       .withColumn(\"end_time\", F.from_utc_timestamp(F.col(\"end_time\"), \"Asia/Kolkata\"))\n",
    "       .write.format(\"delta\")\n",
    "       .mode(\"append\")\n",
    "       .save(\"/mnt/projects/logs/adf_run_logs_tbl\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
